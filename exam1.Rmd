---
title: "exam1"
author: "Mathias Kold"
date: "2024-04-18"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exam 1 - OLS and heteroskedasticity

In a multiple linear regression (MLR) there are 6 assumptions. Assumption 1-5 are called Gauss-Markov assumptions and assumption 6 is called the normality assumption. The first 4 assumptions exist to secure that the model is unbiased, while assumption 5 checks for heteroskedasticity and assumption 6 checks for normality in the model. The assumptions are:
\bold{MLR1: Linear in Parameters}
The model in the population can be written as $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + u$$
where $\beta_0, \beta_1, \ldots, \beta_k$ are the unknown parameters of interest and $u$ is an unobserved random error or disturbance term.

\bold{MLR2: Random Sampling}
We have a random sample of $n$ observations, $\{(x_{i1}, x_{i2}, \ldots, x_{ik}, y_i) : i = 1, 2, \ldots, n\}$, following the population model in Assumption MLR.1.

\bold{MLR3: No Perfect Collinearity}
In the sample (and therefore in the population), none of the independent variables is constant, and there are no exact linear relationships among the independent variables.

\bold{MLR4: Zero Conditional Mean}
The error $u$ has an expected value of zero given any values of the independent variables. In other words, $$E(u | x_1, x_2, \ldots, x_k) = 0$$

\bold{MLR5: Homoskedasticity}
The error $u$ has the same variance given any value of the explanatory variables. In other words, $$\text{Var}(u | x_1, x_2, \ldots, x_k) = \sigma^2$$

\bold{MLR6: Normality}
The population error $u$ is independent of the explanatory variables $x_1, x_2, \ldots, x_k$ and is normally distributed with zero mean and variance $\sigma^2$: $u \sim \text{Normal}(0, \sigma^2)$.

Look at the following model for bank employees wage:
$$log(salary)=\beta_0+\beta_1educ+\beta_2log(salbegin)+\beta_3male+\beta_4 minority+u$$ 
where salary is yearly wage (in 1000 US dollars), educ is education measured in number of years, salbegin is the starting salary (in 1000 US dollars) for the person's first position in the same bank, male is a dummy variable for gender, minority is one dummy variable indicating whether one belongs to a minority.

## 1 - Estimate the model using OLS. Comment on the output and interpret the results
```{r}
options(scipen = 999)
library(readr)
library(foreign)
library(car)
library(sandwich)
library(lmtest)
library(texreg)
data1 <- read_csv("data1.csv")
model <- lm(log(salary) ~ educ + log(salbegin) + male + minority, data = data1)
summary(model)
```

From the estimated model it can be seen that the education variable has a value of 0.02327 which means that one extra year of education will raise the salary by approximately 2.3%. The male variable has a value 0.04816 which means that being a man will raise your salary by approximately 4.8%. The minority variable has a value of -0.04237 which means that minorities approximately will have a 4.2% lower salary than people who are not minorities. The salbegin variable have a different interpretation because it is in log form. It has a value of 0.82180 which means that an increase of 1% in your starting salary will make your salary approximately 0.82% higher.

The intercept is 0.84868 which is the value of the dependent variable, in this case salary, when all other variables have a value of 0. So in this case 0.84868 is the expected value of the salary for a person with zero education, without a starting salary and someone who is not a male or a minority.

It can also be seen that all the variables are statistically significant at a 5% significance level due to p-values < 0.05 but only education and log(salbegin) are statistically significant at a 1% significance level. 

## 2 - Perform graphical model checking.
```{r}
plot(model)
```
The residual vs fitted model shows if the residuals have non-linear patterns. If they are equally spread around a horizontal line without any patterns it indicates that the residuals does not have non-linear patterns. From the plot it can be seen that the spread is quite equal around the horizontal red line, although a few outliers exists, which indicates a linear pattern.

The Q-Q residuals plot shows if the residuals are normally distributed. If the residuals fit the dotted line they are normally distributed. In this case, the residuals fit the dotted line except in the last quantiles where they deviate a bit from the dotted line. This could indicate that the residuals are approximately normally distributed although not perfectly normally distributed.

The scale-location plot shows if homoskedasticity exists within the residuals. If the residuals are spread equally around the predictors, the model fulfills the assumption of homoskedasticity. In this case, it can be argued that there is an increasing pattern in the spread of the residuals which indicates heteroskedasticity.

The residuals vs leverage plot helps detect outliers in the model. It is important because outliers can have a major impact in the model. The residuals can be seen if they have a large Cook's distance, which is the dashed line. In this case, it seems like there might be an outlier with observation 218 but other than that, most of the observations does not have a large Cook's distance. 

## 3 - Test for heteroskedasticity using the Breusch-Pagan test and the special edition of the White test.

First, the test for heteroskedasticity can be carried out by using the Breusch-Pagan test:
```{r}
r=residuals(model)
res = r^2
summary(lm(res~educ + log(salbegin) + male + minority, data = data1))
```
Because the p-value is 0.007475 which is lower than 0.05, we reject the null hypothesis meaning that there is heteroskedasticity in the model.

We can now calculate the LM test, where we have 474 observations and $R^2=0.1231$:
```{r}
LM = 0.1231*474
LM
```
We can then calculate the p-value of chi-square $\chi^2_k$:
```{r}
1-pchisq(LM,4)
```
Both the LM and F test reject the null hypothesis meaning there is heteroskedasticity in the model. The Breusch-Pagan test can also be directly run in R using the bptest function which also gives us the p-value.

```{r}
library(lmtest)
bptest(model)

```
The special White test can also be used to do the test the model for heteroskedasticity. 
```{r}
yhat <- fitted(model)
quadu <- (residuals(model)^2)
model_white <- lm(quadu ~ yhat + I(yhat^2))

whitetest <- c("yhat=0", "I(yhat^2)=0")
linearHypothesis(model_white,whitetest)
```
From both the BP test and the special White test it can be seen that the p-value is < 0.05 which means that there is heteroskedasticity in the model.

## 4 - Calculate robust standard errors for the model and compare with the results in question 1.
Since there is heteroskedasticity in the model, it is necessary to take this into account, when performing a linear regression. A valid estimator of multiple linear regression in the presence of heteroskedasticity is $$Var(\hat{\beta_j})=\frac{\sum\limits_{i=0}^n\hat{r}^2_{ij}\cdot\hat{u}^2_i}{(SSR_j)^2}$$
where $\hat{r}_{ij}$ denotes the $i$th residual from regressing $x_j$ on all other independent variables. The robust standard error is obtained by taking the square root of the above equation. R can calculate the robust standard errors using coeftest():
```{r}
coef_model <- coeftest(model, vcov=vcovHC(model,type="HC0"))
screenreg(list(OLS=model,Standard_Robust_Error=coef_model), digits=4)
```
It can be seen that the estimates from the two models are the same, which is expected, because we try to account for the non-constant variance of the error term in the OLS-model. Hence, it is only the standard errors that change. Furthermore, the significance level does not change, when performing a, when adjusting for heteroskedasticity. The robust standard errors can be used to perform hypothesis testing and to calculate confidence intervals. Even when adjusting for heteroskedasticity, the conclusion from the regressions does not change, since the estimates are the same, and there are no significant changes. 

## 5 - Test the hypothesis $H_0:\beta_2 = 1$ against the alternative $H_1: \beta_2 \neq 1$.
$\beta_2$ is the estimate for the starting salary's effect on the the yearly salary, so when we want to test the null hypothesis $H_0:\beta_2 = 1$ it means that the starting salary has an effect on the yearly salary. (MAYBE COMMENT ON WHAT KIND OF EFFECT!).

To test our null hypothesis against the alternative hypothesis we use the t-statistics, which is given by:
$$t_{{\hat\beta}_j}=\frac{\hat{\beta_j}-1}{se(\hat{\beta_j})}$$
We will perform a two-sided test, so the decision rule will be $|t_{{\hat\beta}_j}|>c$, meaning that the null hypothesis will be rejected if the absolute value of t-statistics is greater than the critical value.

To perform the test we use our estimate $\hat{\beta_2}$ from 1.1.
```{r}
bhat2 <- 0.82180
se_bhat2 <- 0.03603

t_stat <- (bhat2 - 1) / se_bhat2
t_stat
```

Then we need to calculate the critical values for the two-sided test:
```{r}
alpha <- 0.05
c <- qt(1-alpha/2, 469)
c
```
```{r}
abs(t_stat)>c
```


We can see that the absolute value of the t-statistic is greater than the critical value at a 5% significance level, meaning that we reject the null hypothesis, so we instead accept the alternative hypothesis saying that the starting salary does have a statistically significant effect on the yearly salary.

Another way to test the null hypothesis is by calculating the p-value. The definition of the p-value is the probability of obtaining a t-statistic more or at least as extreme than the one observed in the sample. We use R to calculate the p-value in the following way:
```{r}
pval <- 2*pt(-abs(t_stat), 469)
pval
```

So at a 5% significance level the p-value also tells us to reject the null hypothesis, further confirming the rejecting from the t-statistics before. 

## 6 - Test the hypothesis $H_0: \beta_3 = \beta_4 = 0$
When we want to test multiple hypothesis we turn to the F-statistics, where we uses this formula:
$$F=\frac{R^2_{ur}-R^2_{r}}{1-R^2_{ur}}*\frac{n-k-1}{q}$$
where $R^2_{ur}$ is the R-squared from our unrestricted model, $R^2_{r}$ is the R-squared from the restricted, $q$ is the difference in the degrees of freedom between the unrestricted and restricted model, $n$ is the number of observations in the dataset and $k$ is the number of independent variables in the unrestricted model.

In our case the unrestricted model is the one given in the beginning of assignment 1.1 with all the independent variables, and in the case where we want to test the null hypothesis $H_0: \beta_3 = \beta_4 = 0$, we then get our restricted model:
$$log(salary)=\beta_0+\beta_1educ+\beta_2log(salbegin)+u$$

So we start by estimating our restricted model:
```{r}
model_r <- lm(log(salary) ~ educ + log(salbegin), data = data1)
```

We then obtain our $R^2$ from the restricted and unrestriced model:
```{r}
r2_ur <- summary(model)$r.squared
r2_r <- summary(model_r)$r.squared
```

We now have what we need to calculate our F-statistic:
```{r}
F <- (r2_ur-r2_r)/(1-r2_ur) * (474-4-1)/2
F
```

We then calculate our critical values of the F-statistics at 5% significance level:
```{r}
qf(0.95, 2, 469)
```

Our decision rule says that if F > c then we reject our null hypothesis and instead accepting our alternative hypothesis meaning that $\beta_3$ and $\beta_4$ does have a statistically impact on yearly salary. 

It is also possible to do the test directly in R by using the following code:
```{r}
myh0 <- c("male=0", "minority=0")
linearHypothesis(model, myh0)
```

Using the code in R to perform the F-statistics gives us the some answer as before still confirming the rejecting of our null hypothesis.


## 7 - Estimate the model using FGLS and comment on the results.
Feasible generalised least squares (FGLS) is a method used to adress heteroskedasticity. It can difficult to know the form of heteroskedasticity (i.e, h($x_i$)). In many cases, it is possible to model the function $h$ and use this data to estimate the unknown parameters. By estimating $h_i$, an estimated value of $h$ is obtained, $\hat{h_i}$. One method to find the exact form of $h_i$ is as follows: $$Var(u|x)=\sigma^2exp(\delta_0+\delta_1x_1+\delta_2x_2+...+\delta_kx_k)$$
where $x_1,x_2,...,x_k$ are the independent variables in the regression.

Since the parameters $\delta$ is unknown for the population, these are estimated using the given data, where $\hat{h}$ can be used to account for heteroskedasticity. To find the estimates for $\delta$, we can use the following. By taking log, the model can be linearised. 

$$u^2=\sigma^2exp(\delta_0+\delta_1x_1+\delta_2x_2+...+\delta_kx_k)v$$


$$log(u^2)=a_0+\delta_1x_1+\delta_2x_2+...+\delta_kx_k+e$$


We already know from 1.3 that our model has heteroskedasticity problems, so we obtain our squared residuals from our OLS model and log them:
```{r}
logu2 <- log(resid(model)^2)
```

So now we run the regression on the form mentioned earlier:
```{r}
varreg <- lm(logu2 ~ educ + log(salbegin) + male + minority, data = data1)
```

After we calculate the weights by exponentiating the fitted values from varreg:
```{r}
w <- exp(fitted(varreg))
```

And then we can estimate the FGLS:
```{r}
FGLS <- lm(log(salary) ~ educ + log(salbegin) + male + minority, weight=1/w, data = data1)
screenreg(list(OLS=model, FLGS=FGLS), digits=4)
```
It can be seen that the standard error for the different variables have been changed a bit, but nothing significant. It is very small changes, which maybe could indicate that the FGLS estimation have not taken into account all the heteroskedasticity. This will be elaborated further in 1.8.

## 8 - Has the FGLS estimation taken into account all the heteroskedasticity?
There are more ways to check whether the FGLS estimation has taken all the heteroskedasticity into account. One way is to look at it graphically and another is by doing a BP-test.

We start by looking at it graphically in the Scale-Location plot:
```{r}
plot(FGLS, 3)
```

Then we perform a BP-test:
```{r}
bptest(FGLS)
```

Both the plot and BP-test indicates that the FGLS estimation have not taken all of the heteroskedasticity into account. In the plot it looks like there is a slight upward trend and that the spread of the residuals increase with higher fitted values. This suggest that the variance of the residuals is not constant indicating heteroskedasticity. As mentioned in 1.3 the null hypothesis in a BP-test is homoskedasticity, but we reject the null hypothesis here, since p-value is > 0.05.

