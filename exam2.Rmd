---
title: "Exam 2"
author: "Mathias Kold"
date: "2024-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exam 2 - OLS and misspecification

In a multiple linear regression (MLR) there are 6 assumptions. Assumption 1-5 are called Gauss-Markov assumptions and assumption 6 is called the normality assumption. The first 4 assumptions exist to secure that the model is unbiased, while assumption 5 checks for heteroskedasticity and assumption 6 checks for normality in the model. The assumptions are:

**MLR1:** Linear in Parameters
The model in the population can be written as $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + u$$
where $\beta_0, \beta_1, \ldots, \beta_k$ are the unknown parameters of interest and $\u$ is an unobserved random error or disturbance term.

**MLR2:** Random Sampling
We have a random sample of $\n$ observations, $\{(x_{i1}, x_{i2}, \ldots, x_{ik}, y_i) : i = 1, 2, \ldots, n\}$, following the population model in Assumption MLR.1.

**MLR3:** No Perfect Collinearity
In the sample (and therefore in the population), none of the independent variables is constant, and there are no exact linear relationships among the independent variables.

**MLR4:** Zero Conditional Mean
The error $\u$ has an expected value of zero given any values of the independent variables. In other words, $$E(u | x_1, x_2, \ldots, x_k) = 0$$

**MLR5:** Homoskedasticity
The error $\u$ has the same variance given any value of the explanatory variables. In other words, $$\text{Var}(u | x_1, x_2, \ldots, x_k) = \sigma^2$$

**MLR6:** Normality
The population error $\u$ is independent of the explanatory variables $x_1, x_2, \ldots, x_k$ and is normally distributed with zero mean and variance $\sigma^2$: $u \sim \text{Normal}(0, \sigma^2)$.

Consider the following two models for bank employees' salaries:
\begin{enumerate}
    \item $salary = \beta_0 + \beta_1 educ + \beta_2 salbegin + \beta_3 male + \beta_4 minority + u$ (1)
    \item $\log(salary) = \beta_0 + \beta_1 educ + \beta_2 \log(salbegin) + \beta_3 male + \beta_4 minority + u$ (2)
\end{enumerate}
where $salary$ is the annual salary (in 1000 US dollars), $educ$ is education measured in number of years, $salbegin$ is the starting salary (in 1000 US dollars) for the person's first position in the same bank, $male$ is a dummy variable for gender, and $minority$ is a dummy variable indicating whether one belongs to a minority.


```{r}
library(readr); library(texreg)
data2 <- read_csv("data2.csv")
educ <- data2$educ; salbegin <- data2$salbegin; male <- data2$male; minority <- data2$minority; salary <- data2$salary
```


## 1. Estimate the two models using OLS. Comment on the output, compare and interpret the results.

Here we use the lm() function to estimate the two models. 
```{r}
library(lmtest)
model = lm(salary ~ educ + salbegin + male + minority, data = data2)
model2 = lm(log(salary) ~ educ + log(salbegin) + male + minority, data = data2)
```

```{r}
summary(model)
```
From the first model it can be seen that all variables except for "minority" are significant at a 5% level. The intercept is almost -7, and minority affects salary negative as well. The rest of the variables will affect the salary positive.
The dummy variables show that men earn 1.83 dollars more than females and minorities earn 1.73 less than non-minorities.

```{r}
summary(model2)
```
From model 2 it can be seen that all variables are significant at a 5% level, and intercept, education and log(salbegin) are significant at a .1% level.
All variables except minority is positive. 
The dummy variables show that men earn 4.5% more than female and minorities earn 4.2% less than non-minorities.

It is worth noting that the adjusted R-squared is 0.8034 for the second model, where as it is a bit lower in the first model at 0.7944.

## 2. Carry out graphical model checking of the two models. Which model would you prefer?
```{r}
plot(model, 2)
```

```{r}
plot(model2, 2)
```
It seems like model 2 is the better model to describe salary, since the residuals are closer to the linear regression and there are fewer outliers.


## 3. Examine whether the two models are misspecified using the RESET test.
The RESET test tests whether there are any non-linear relations in the data, that the original model(s) don't capture. The RESET test accounts for the degrees of freedom by adding non-linear forms of fitted values of Y to the original model, and the significance of the coeffients are tested in an F-test.

The nullhypothesis, $H_0$ is that the model is correctly specified:
$$H_0:\delta_1=0=,\delta_2=0$$
And the alternative hypothesis is:
$$H_1:\delta_1\neq0,\delta_2=0$$
First we use the values found from the two models to find our yhat for model and model2.
Setting up the new models to perform a RESET test:
$y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k+\delta_1\hat{y}^2+\delta_2\hat{y}^3+v$
where $\hat{y}$ is the fitted value from the original model. $\hat{y}^2$ and $\hat{y}^3$ will capture any nonlinearities, if present in the model.
```{r}
yhat1 <- -6.93228 + 0.99327*educ + 1.60816*salbegin + 1.83088*male - 1.72539*minority
yhat2 <- 0.849130 + 0.023578*log(salbegin) + 0.045474*male - -0.041856*minority
yhat1_sq <- yhat1^2; yhat1_cub <- yhat1^3
yhat2_sq <- yhat2^2; yhat2_cub <- yhat2^3
mod1_res <- lm(salary ~ educ + salbegin + male + minority + yhat1_sq + yhat1_cub)
mod2_res <- lm(log(salary) ~ educ + log(salbegin) + male + minority + yhat2_sq + yhat2_cub)
```

When the the RESET models has been made, we can use waldtest to find the F-statistics: 
```{r}
waldtest(mod1_res, terms=c("yhat1_sq","yhat1_cub"))
```
For model2:
```{r}
library(lmtest)
waldtest(mod2_res, terms=c("yhat2_sq","yhat2_cub"))
```

To confirm our results, we can use (resettest)
```{r}
resettest(model); resettest(model2)
```
Since both models have a p-value higher than 5%, respectively 7,7% and 7,3%, $H_0$ cannot be rejected, and therefore we cannot conclude that any of the models are mis-specified. However, both p-values are still pretty low, and would not be accepted at e.g. a 10% significance level.


## 4. Explain why it could be relevant to include $educ^2$ as an explanatory variable in the two models. Estimate the two models again with $educ^2$ included (along with its corresponding coefficient $β_5$). Briefly comment on the output, and perform the RESET test again.


If it is still assumed, that the model is mis-specified, then it could be argued that leaving out the squared 

```{r}
model_ed <- lm(salary~educ+salbegin+male+minority+I(educ^2))
summary(model_ed)
```


If it is still assumed, that the model is mis-specified, then it could be argued that leaving out the squared 

```{r}
model_ed <- lm(salary~educ+salbegin+male+minority+I(educ^2))
summary(model_ed)
```

```{r}
model_ed2 <- lm(log(salary)~educ+log(salbegin)+male+minority+I(educ^2))
summary(model_ed2)
```

```{r}
resettest(model_ed); resettest(model_ed2)
```
Here it can be seen that the p-value increases, when $educ^2$ is included, and therefore we can still not reject $H_0$. The non-linear forms of the fitted values are not statistically significant.


We can set up comparisons of the two models, with and without $educ^2$
```{r}
screenreg(list(Model1=model, Model1_ed2=model_ed))
```

In the above lists, there's an upward bias on educ, when $educ^2$ is not included, whereas there's a negative effect when $educ^2$ is included, while education in the long run will affect your salary positive. 

Comparing for log(salary):
```{r}
screenreg(list(Model2=model2, Model2_ed2=model_ed2), digits=5)
```
For these, the same pattern can be observed. Education will affect negatively for short education, while a long education will affect your salary positively. 

## 5. Test the hypothesis $H_0: β_1 = β_5 = 0$ in both models (from question 4).

To test the hypothesis $H_0: β_1 = β_5 = 0$ in both models from question 4, we can use the F-test. This tests whether both the coefficient for educ and the coefficient for $educ^2$ are equal to zero. If the null hypothesis is rejected, it indicates that at least one of these coefficients is statistically significant.

The formula for the F-statistics is:
$$F=\frac{R^2_{ur}-R^2_{r}}{1-R^2_{ur}}*\frac{n-k-1}{q}$$
where $R^2_{ur}$ is the R-squared from our unrestricted model, $R^2_{r}$ is the R-squared from the restricted, $q$ is the difference in the degrees of freedom between the unrestricted and restricted model, $n$ is the number of observations in the dataset and $k$ is the number of independent variables in the unrestricted model.

The two unrestricted models are given in exercise 2.4, so what we need to do is estimate our two restricted models, that looks like the following:
$$salary=\beta_0+\beta_2salbegin+\beta_3male+\beta_4minority+u$$
$$log(salary)=\beta_0++\beta_2log(salbegin)+\beta_3male+\beta_4minority+u$$
```{r}
model_r <- lm(salary ~ salbegin + male + minority, data = data2)
model_r2 <- lm(log(salary) ~ log(salbegin) + male + minority, data = data2)
```

We then obtain our $R^2$ from the restricted and unrestriced models:
```{r}
r2_ur <- summary(model_ed)$r.squared
r2_r <- summary(model_r)$r.squared

r2_ur2 <- summary(model_ed2)$r.squared
r2_r2 <- summary(model_r2)$r.squared
```

We now have what we need to calculate our F-statistics:
```{r}
F1 <- (r2_ur-r2_r)/(1-r2_ur) * (450-5-1)/2
F2 <- (r2_ur2-r2_r2)/(1-r2_ur2) * (450-5-1)/2
F1
F2
```

We then calculate our critical values of the F-statistics at 5% significance level:
```{r}
qf(0.95, 2, 450-5-1)
```

Our decision rule says that if F > c then we reject our null hypothesis and instead accepting our alternative hypothesis meaning that $\beta_1$ and $\beta_5$ does have a statistically impact on yearly salary in both of our models. 

It is also possible to do the test directly in R by using the following code:
```{r}
myh0 <- c("educ=0", "I(educ^2)=0")
linearHypothesis(model_ed, myh0)
linearHypothesis(model_ed2, myh0)

```

Using the code in R to perform the F-statistics gives us the some answer as before still confirming the rejecting of our null hypothesis.


## 6. Could there be issues with measurement errors in the two models? In what cases would it pose a problem?


