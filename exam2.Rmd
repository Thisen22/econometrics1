---
title: "Exam 2"
author: "Mathias Kold"
date: "2024-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exam 2 - OLS and misspecification

In a multiple linear regression (MLR) there are 6 assumptions. Assumption 1-5 are called Gauss-Markov assumptions and assumption 6 is called the normality assumption. The first 4 assumptions exist to secure that the model is unbiased, while assumption 5 checks for heteroskedasticity and assumption 6 checks for normality in the model. The assumptions are:

**MLR1:** Linear in Parameters
The model in the population can be written as $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + u$$
where $\beta_0, \beta_1, \ldots, \beta_k$ are the unknown parameters of interest and $\u$ is an unobserved random error or disturbance term.

**MLR2:** Random Sampling
We have a random sample of $\n$ observations, $\{(x_{i1}, x_{i2}, \ldots, x_{ik}, y_i) : i = 1, 2, \ldots, n\}$, following the population model in Assumption MLR.1.

**MLR3:** No Perfect Collinearity
In the sample (and therefore in the population), none of the independent variables is constant, and there are no exact linear relationships among the independent variables.

**MLR4:** Zero Conditional Mean
The error $\u$ has an expected value of zero given any values of the independent variables. In other words, $$E(u | x_1, x_2, \ldots, x_k) = 0$$

**MLR5:** Homoskedasticity
The error $\u$ has the same variance given any value of the explanatory variables. In other words, $$\text{Var}(u | x_1, x_2, \ldots, x_k) = \sigma^2$$

**MLR6:** Normality
The population error $\u$ is independent of the explanatory variables $x_1, x_2, \ldots, x_k$ and is normally distributed with zero mean and variance $\sigma^2$: $u \sim \text{Normal}(0, \sigma^2)$.

Consider the following two models for bank employees' salaries:
\begin{enumerate}
    \item $salary = \beta_0 + \beta_1 educ + \beta_2 salbegin + \beta_3 male + \beta_4 minority + u$ (1)
    \item $\log(salary) = \beta_0 + \beta_1 educ + \beta_2 \log(salbegin) + \beta_3 male + \beta_4 minority + u$ (2)
\end{enumerate}
where $salary$ is the annual salary (in 1000 US dollars), $educ$ is education measured in number of years, $salbegin$ is the starting salary (in 1000 US dollars) for the person's first position in the same bank, $male$ is a dummy variable for gender, and $minority$ is a dummy variable indicating whether one belongs to a minority.


```{r}
library(readr); library(texreg)
data2 <- read_csv("data2.csv")
educ <- data2$educ; salbegin <- data2$salbegin; male <- data2$male; minority <- data2$minority; salary <- data2$salary
```


##1. Estimate the two models using OLS. Comment on the output, compare and interpret the results.

Here we use the lm() function to estimate the two models. 
```{r}
library(lmtest)
model = lm(salary ~ educ + salbegin + male + minority, data = data2)
model2 = lm(log(salary) ~ educ + log(salbegin) + male + minority, data = data2)
```

```{r}
summary(model)
```
The education variable has an estimate of 0.99327 which means that one extra year of education will raise the annual salary by approximately 993 dollars. The salbegin variable has an estimate of 1.60816 which means that one more unit (1000 dollars) of starting salary will give 1.60816 more units (1608.16 dollars) of annual salary. The male variable has an estimate of 1.83088 which means that if you are a male your annual salary will be 1.83088 units (1830.88 dollars) higher than if you are a woman. The minority variable has an estimate of -1.72539 which means that if you are a minority your annual salary will be 1.72539 units (1725.39 dollars) lower than if you are not a minority.

The intercept is -6.93228 which is the value of the dependent variable, in this case annual salary (in 1000 dollars), when all other variables have a value of 0. In this case a negative intercept does not really make sense since salary can not be negative.

It can also be seen that all the variables except minority variable are statistically significant at a 5% significance level due to p-values < 0.05. Education and salbegin are also significant at a 1% significance level while the male variable is not.

```{r}
summary(model2)
```
In the second model, it can be seen that the education variable has an estimate of 0.023578 which means that one extra year of education will raise the annual salary by approximately 2.36%. The log(salbegin) variable has an estimate of 0.820725. Since it is in log form, it means that a 1% increase in the starting salary will raise the annual salary by approximately 0.821%. The male variable has an estimate of 0.045474 which means that your annual salary will be approximately 4.55% higher if you are a male than if you are a woman. The minority variable has an estimate of -0.041856 which means that if you are a minority your annual salary will be approximately 4.19% lower than if you are not a minority.

The intercept is 0.84913 which is the value of the dependent variable, in this case annual salary (in 1000 dollars), when all other variables have a value of 0. 

It can also be seen that all the variables are statistically significant at a 5% significance level due to p-values < 0.05. Only education and log(salbegin) are significant at a 1% significance level.

It is worth noting that the adjusted R-squared is 0.8034 for the second model, where as it is a bit lower in the first model at 0.7944.

##2. Carry out graphical model checking of the two models. Which model would you prefer?
```{r}
plot(model)
```
The residual vs fitted model shows if the residuals have non-linear patterns. If they are equally spread around a horizontal line without any patterns it indicates that the residuals does not have non-linear patterns. It can be seen that the residuals are not quite spread around a horizontal line but rather a decreasing line.

The Q-Q residuals plot shows if the residuals are normally distributed. If the residuals fit the dotted line they are normally distributed. In this case, the residuals fit the dotted line except in the first and last quantiles where they deviate from the dotted line. They deviate a bit more upwards from the line than downwards. This could indicate that the residuals are approximately normally distributed although not perfectly normally distributed.

The scale-location plot shows if homoskedasticity exists within the residuals. If the residuals are spread equally around the predictors, the model fulfills the assumption of homoskedasticity. In this case, it can be argued that there is an increase in the spread of the residuals which indicates heteroskedasticity.

The residuals vs leverage plot helps detect outliers in the model. It is important because outliers can have a major impact in the model. The residuals can be seen if they have a large Cook's distance, which is the dashed line. In this case, it does not seem like there is any observations with a large enough Cook's distance to be outside of the limits. There are some observations quite far apart from the rest but none outside of the limits from the dashed lines.

```{r}
plot(model2)
```
From the residuals vs fitted plot it can be seen that the residuals are more equally spread around the line than in the first model. The line also looks to be more horizontal where the first model showed a more decreasing line.

From the Q-Q residuals plot it can be seen that the residuals fit the dotted line better than the first model although still not perfectly. This indicates that the residuals are approximately normally distributed and at least closer to a normal distribution than the first model.

The scale-location plot does not show the same signs of heteroskedasticity as in the first model. The residuals are closer together and are spread more equally around the horizontal line in this model.

The residuals vs leverage plot does not show any significant outliers as none of the observations have a large enough Cook's distance to be outside of the limits.

Because it fits the four models the best, it can be argued that model 2 would be preferred over model 1.

##3. Examine whether the two models are misspecified using the RESET test.
The RESET test tests whether there are any non-linear relations in the data, that the original model(s) don't capture. The RESET test accounts for the degrees of freedom by adding non-linear forms of fitted values of Y to the original model, and the significance of the coeffients are tested in an F-test.

The null hypothesis, $H_0$ is that the model is correctly specified:
$$H_0:\delta_1=0=,\delta_2=0$$
And the alternative hypothesis is:
$$H_1:\delta_1\neq0,\delta_2=0$$
First we use the values found from the two models to find our yhat for model and model2.
Setting up the new models to perform a RESET test:
$$y=\beta_0+\beta_1x_1+\beta_2x_2+...+\beta_kx_k+\delta_1\hat{y}^2+\delta_2\hat{y}^3+v$$
where $\hat{y}$ is the fitted value from the original model. $\hat{y}^2$ and $\hat{y}^3$ will capture any nonlinearities, if present in the model.
```{r}
yhat1 <- -6.93228 + 0.99327*educ + 1.60816*salbegin + 1.83088*male - 1.72539*minority
yhat2 <- 0.849130 + 0.023578*log(salbegin) + 0.045474*male - -0.041856*minority
yhat1_sq <- yhat1^2; yhat1_cub <- yhat1^3
yhat2_sq <- yhat2^2; yhat2_cub <- yhat2^3
mod1_res <- lm(salary ~ educ + salbegin + male + minority + yhat1_sq + yhat1_cub)
mod2_res <- lm(log(salary) ~ educ + log(salbegin) + male + minority + yhat2_sq + yhat2_cub)
```

When the the RESET models has been made, we can use waldtest to find the F-statistics: 
```{r}
waldtest(mod1_res, terms=c("yhat1_sq","yhat1_cub"))
```
For model2:
```{r}
library(lmtest)
waldtest(mod2_res, terms=c("yhat2_sq","yhat2_cub"))
```

To confirm our results, we can use (resettest)
```{r}
resettest(model); resettest(model2)
```
Since both models have a p-value higher than 5%, respectively 7,7% and 7,3%, $H_0$ cannot be rejected, and therefore we cannot conclude that any of the models are mis-specified. However, both p-values are still pretty low, and would not be accepted at e.g. a 10% significance level.


##4. Explain why it could be relevant to include $educ^2$ as an explanatory variable in the two models. Estimate the two models again with $educ^2$ included (along with its corresponding coefficient $β_5$). Briefly comment on the output, and perform the RESET test again.


If it is still assumed, that the model is mis-specified, then it could be argued that including the squared could help correctly specifying the model. Excluding the variable could lead to a bias on educ in the original model. Furthermore, by including $educ^2$, the model will be able to capture any quadratic forms. 

```{r}
model_ed <- lm(salary~educ+salbegin+male+minority+I(educ^2))
summary(model_ed)
```


```{r}
model_ed2 <- lm(log(salary)~educ+log(salbegin)+male+minority+I(educ^2))
summary(model_ed2)
```

```{r}
resettest(model_ed); resettest(model_ed2)
```
Here it can be seen that the p-value increases, when $educ^2$ is included, and therefore we can still not reject $H_0$. The non-linear forms of the fitted values are not statistically significant.


We can set up comparisons of the two models, with and without $educ^2$
```{r}
screenreg(list(Model1=model, Model1_ed2=model_ed))
```

In the above lists, there's an upward bias on educ, when $educ^2$ is not included, whereas there's a negative effect when $educ^2$ is included, while education in the long run will affect your salary positive. 

Comparing for log(salary):
```{r}
screenreg(list(Model2=model2, Model2_ed2=model_ed2), digits=5)
```
For these, the same pattern can be observed. Education will affect negatively for short education, while a long education will affect your salary positively. Both models has a convex shape, where education will affect negatively on the short run, but there is an accelerating effect, as you keep studying. By performing RESET tests for the models including $educ^2$, we can find out whether the models are better specified.
```{r}
resettest(model_ed);resettest(model_ed2)
```
Since the p-values increase, the models are now better specified, and $H_0$ cannot be rejected.

##5. Test the hypothesis $H_0: β_1 = β_5 = 0$ in both models (from question 4).

To test the hypothesis $H_0: β_1 = β_5 = 0$ in both models from question 4, we can use the F-test. This tests whether both the coefficient for educ and the coefficient for $educ^2$ are equal to zero. If the null hypothesis is rejected, it indicates that at least one of these coefficients is statistically significant.


##6. Could there be issues with measurement errors in the two models? In what cases would it pose a problem?


